{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning practice\n",
    "## Implementing the SARSA on-policy algorithm for learning the optimal policy\n",
    "Let's consider a simple 3x3 grid world scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "player = \"P\"\n",
    "goal = \"G\"\n",
    "initial_state = 0\n",
    "final_state = N**2 - 1\n",
    "\n",
    "\n",
    "states = [0 for i in range(0,N**2)]\n",
    "states[initial_state] = player\n",
    "states[final_state] = goal\n",
    "visited_states = [initial_state]\n",
    "\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "learning_history = [states.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_maze(states):\n",
    "    print(\"\")\n",
    "    print(states[:N])\n",
    "    for i in range(1,N-1):\n",
    "        print(states[i*N:(i+1)*N])\n",
    "    print(states[N**2-N:])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_position(states, visited_states, learning_history):\n",
    "    states = [0 for i in range(0,N**2)]\n",
    "    states[initial_state] = player\n",
    "    states[final_state] = goal\n",
    "    visited_states = [initial_state]\n",
    "    learning_history = [states.copy()]\n",
    "    return states, visited_states, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze:\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"maze:\")\n",
    "print_maze(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 1\n",
    "wall_reward = -1\n",
    "repeated_state_reward = -1\n",
    "step_reward = -0.1\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_model(N):\n",
    "    positions = range(0,N**2)\n",
    "    transition_model = {position: {} for position in positions}\n",
    "\n",
    "    for position in positions:\n",
    "        transition_model[position][\"up\"] = position - N\n",
    "        transition_model[position][\"down\"] = position + N\n",
    "        transition_model[position][\"right\"] = position + 1\n",
    "        transition_model[position][\"left\"] = position - 1\n",
    "        if position < N:\n",
    "            transition_model[position][\"up\"] = position\n",
    "        if position in range(N**2 - N, N**2):\n",
    "            transition_model[position][\"down\"] = position\n",
    "        if position % N == 0:\n",
    "            transition_model[position][\"left\"] = position\n",
    "        if (position + 1) % N == 0:\n",
    "            transition_model[position][\"right\"] = position\n",
    "    \n",
    "    return transition_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(N):\n",
    "    positions = range(0,N**2)\n",
    "    q_table = {position: {action: 0 for action in actions} for position in positions}\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = build_transition_model(N)\n",
    "q_table = build_q_table(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(current_state, verbose=True):\n",
    "    log_print = []\n",
    "    # let's choose an action with Îµ greedy\n",
    "    if verbose:\n",
    "        print(f\"q scores: {q_table.get(current_state)}\")\n",
    "        log_print.append(f\"q scores: {q_table.get(current_state)}\")\n",
    "    \n",
    "    epsilon = 0.05\n",
    "    sample = random.uniform(0,1)\n",
    " \n",
    "    next_action = random.choice(actions) # setting an initial random action\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    \n",
    "    # for now my next_action is the best according to greedy selection.\n",
    "    \n",
    "    if sample > epsilon:\n",
    "        # greedy case\n",
    "        next_action = next_action\n",
    "        if verbose:\n",
    "            print(f\"greedy action: {next_action}\")\n",
    "            log_print.append(f\"greedy action: {next_action}\")\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        actions_without_the_best = [action for action in actions if action != next_action]\n",
    "        next_action = random.choice(actions_without_the_best)\n",
    "        best_q_value = q_table.get(current_state).get(next_action)\n",
    "        if verbose:\n",
    "            print(f\"random action: {next_action}\")\n",
    "            log_print.append(f\"random action: {next_action}\")\n",
    "    \n",
    "    print(\"\")\n",
    "    return next_action, best_q_value, log_print\n",
    "    \n",
    "\n",
    "def get_reward(current_state, new_state):\n",
    "    log_print = \"\"\n",
    "    if new_state == final_state:\n",
    "        reward = goal_reward\n",
    "        print(\"I reached the goal!\")\n",
    "        log_print += \"I reached the goal!\"\n",
    "\n",
    "    elif current_state == new_state:\n",
    "        reward = wall_reward\n",
    "        print(\"I hit the wall!\")\n",
    "        log_print += \"I hit the wall!\"\n",
    "\n",
    "    elif new_state in visited_states:\n",
    "        print(\"I have already visited this state!\")\n",
    "        reward = repeated_state_reward\n",
    "        log_print += \"I have already visited this state!\"\n",
    "    \n",
    "    elif current_state != new_state:\n",
    "        print(\"I made a step!\")\n",
    "        reward = step_reward\n",
    "        log_print += \"I have already visited this state!\"\n",
    "\n",
    "    else:\n",
    "        print(\"what is going on???\")\n",
    "        exit(0)\n",
    "        \n",
    "    \n",
    "    return reward, [log_print]\n",
    "\n",
    "def move_player(current_state, new_state):\n",
    "    states[current_state] = 0\n",
    "    states[new_state] = player\n",
    "    learning_history.append(states.copy())\n",
    "    if new_state not in visited_states:\n",
    "        visited_states.append(new_state)\n",
    "\n",
    "\n",
    "def get_best_q_value(current_state):\n",
    "    next_action = actions[0]\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions[1:]:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    return best_q_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "I'm in position 0 and the episode just started.\n",
      "I will select right as action\n",
      "From now on i will use what i learn to move through the maze.\n",
      "\n",
      "I'm moving to the position 1\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "\n",
      "[0, 'P', 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: down\n",
      "\n",
      "From 1, i select down as the next action\n",
      "I'm moving to the position 4\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 'P', 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "From 4, i select right as the next action\n",
      "I'm moving to the position 5\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 'P']\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "From 5, i select left as the next action\n",
      "I'm moving to the position 4\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 'P', 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': -0.05}\n",
      "greedy action: up\n",
      "\n",
      "From 4, i select up as the next action\n",
      "I'm moving to the position 1\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 'P', 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': -0.05, 'left': 0, 'right': 0}\n",
      "greedy action: up\n",
      "\n",
      "From 1, i select up as the next action\n",
      "I'm moving to the position 1\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "[0, 'P', 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': -0.05, 'left': 0, 'right': 0}\n",
      "greedy action: up\n",
      "\n",
      "From 1, i select up as the next action\n",
      "I'm moving to the position 1\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "[0, 'P', 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "From 1, i select left as the next action\n",
      "I'm moving to the position 0\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': -0.05}\n",
      "greedy action: down\n",
      "\n",
      "From 0, i select down as the next action\n",
      "I'm moving to the position 3\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "From 3, i select left as the next action\n",
      "I'm moving to the position 3\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "From 3, i select left as the next action\n",
      "I'm moving to the position 3\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': 0}\n",
      "greedy action: up\n",
      "\n",
      "From 3, i select up as the next action\n",
      "I'm moving to the position 0\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: up\n",
      "\n",
      "From 0, i select up as the next action\n",
      "I'm moving to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: up\n",
      "\n",
      "From 0, i select up as the next action\n",
      "I'm moving to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "From 0, i select left as the next action\n",
      "I'm moving to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "From 0, i select left as the next action\n",
      "I'm moving to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': -0.5, 'right': -0.05}\n",
      "random action: up\n",
      "\n",
      "From 0, i select up as the next action\n",
      "I'm moving to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': -0.5225, 'right': -0.05}\n",
      "greedy action: down\n",
      "\n",
      "From 0, i select down as the next action\n",
      "I'm moving to the position 3\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': -0.5, 'right': 0}\n",
      "greedy action: down\n",
      "\n",
      "From 3, i select down as the next action\n",
      "I'm moving to the position 6\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "From 6, i select right as the next action\n",
      "I'm moving to the position 7\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 'P', 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "From 7, i select left as the next action\n",
      "I'm moving to the position 6\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': -0.05}\n",
      "greedy action: up\n",
      "\n",
      "From 6, i select up as the next action\n",
      "I'm moving to the position 3\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': -0.5, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "From 3, i select right as the next action\n",
      "I'm moving to the position 4\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 'P', 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': 0, 'right': -0.05}\n",
      "greedy action: down\n",
      "\n",
      "From 4, i select down as the next action\n",
      "I'm moving to the position 7\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 'P', 'G']\n",
      "\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "From 7, i select right as the next action\n",
      "I'm moving to the position 8\n",
      "I reached the goal!\n",
      "reward: 1\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'P']\n",
      "\n",
      "\n",
      "Episode time (iterations): 24\n"
     ]
    }
   ],
   "source": [
    "states, visited_states, learning_history = reset_position(states, visited_states, learning_history)\n",
    "print_maze(states)\n",
    "\n",
    "# algorithm = \"sarsa\"\n",
    "algorithm = \"q-learning\"\n",
    "\n",
    "training_time = range(0,1000)\n",
    "current_state = initial_state\n",
    "current_action, old_q_estimate = choose_action(current_state)\n",
    "\n",
    "print(f\"I'm in position {current_state} and the episode just started.\")\n",
    "print(f\"I will select {current_action} as action\")\n",
    "print(\"From now on i will use what i learn to move through the maze.\")\n",
    "print(\"\")\n",
    "\n",
    "for t in training_time: # for each episode\n",
    "    # choose the action according to the q_table\n",
    "    \n",
    "    new_state = transition_model.get(current_state).get(current_action)\n",
    "    print(f\"I'm moving to the position {new_state}\")\n",
    "    reward = get_reward(current_state, new_state)\n",
    "    print(f\"reward: {reward}\")\n",
    "    \n",
    "    move_player(current_state, new_state)\n",
    "    print_maze(states)\n",
    "\n",
    "    new_action, next_q_estimate = choose_action(new_state, verbose=(not new_state==final_state))\n",
    "\n",
    "    if algorithm == \"sarsa\":\n",
    "        updated_q_estimate = old_q_estimate + alpha*(reward + gamma*next_q_estimate - old_q_estimate)\n",
    "    if algorithm == \"q-learning\":\n",
    "        updated_q_estimate = old_q_estimate + alpha*(reward + gamma*get_best_q_value(current_state) - old_q_estimate)\n",
    "\n",
    "    q_table[current_state][current_action] = updated_q_estimate\n",
    "\n",
    "    current_state = new_state\n",
    "    current_action, old_q_estimate = new_action, next_q_estimate\n",
    "\n",
    "    if current_state == N**2-1:\n",
    "        break\n",
    "    \n",
    "    print(f\"From {new_state}, i select {new_action} as the next action\", end='\\n')\n",
    "\n",
    "print(f\"Episode time (iterations): {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line2D(_child1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAD3ElEQVR4nO3YvY0bVxiG0Y8UnUhgAwRZhUtwDYY7UA+uSQU4cgmqggs2QICRlqPIPwkhQuJisXrOSecGb/Jg5s5qWZZlgJ/a+rUHAC9P6BAgdAgQOgQIHQKEDgFChwChQ8DmnkPX63VOp9Nst9tZrVYvvQm407Iscz6fZ7fbzXp9+719V+in02kOh8PDxgGPdTweZ7/f33x+V+jb7XZmZn797dO827x/zDLghz1/ucznv3//t9Fb7gr9n8/1d5v3s/nlw4+vAx7qW1dqP+MgQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQoeAzWsPgP/786+Prz3hTbksz/PHHee80SFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAgQOgQIHQKEDgFChwChQ4DQIUDoECB0CBA6BAgdAoQOAUKHAKFDgNAhQOgQIHQIEDoECB0ChA4BQocAoUOA0CFA6BAgdAjY3HNoWZaZmXn+cnnRMXBZnl97wptyWa4z81+jt6yWb52YmaenpzkcDo9ZBjzc8Xic/X5/8/ldoV+v1zmdTrPdbme1Wj10IPD9lmWZ8/k8u91u1uvbN/G7QgfeNj/jIEDoECB0CBA6BAgdAoQOAUKHgK+ON1AbOpQpewAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "# def plot_grid(learning_history):\n",
    "\n",
    "grids = []\n",
    "\n",
    "goal = learning_history[0].index(\"G\")\n",
    "\n",
    "for maze in learning_history:\n",
    "    player = learning_history[-1].index(\"P\")\n",
    "    grid = np.zeros((3, 3))\n",
    "    x, y = divmod(player, 3)  \n",
    "    gx, gy = divmod(goal, 3)\n",
    "\n",
    "    grid[x, y] = 1  # Posizione del giocatore\n",
    "    grid[gx, gy] = 2  # Posizione del goal\n",
    "    grids.append(grid.copy())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))  \n",
    "ax.imshow(grids[-1], cmap=\"coolwarm\", origin=\"upper\")\n",
    "ax.set_xticks([]), ax.set_yticks([])\n",
    "animated_plot, = ax.plot([],[])\n",
    "print(animated_plot)\n",
    "plt.show()\n",
    "\n",
    "def update_data(frame):\n",
    "    animated_plot.set_data(grid[frame])\n",
    "    return animated_plot\n",
    "\n",
    "animation = FuncAnimation(\n",
    "    fig=fig,\n",
    "    func=update_data,\n",
    "    frames=len(learning_history),\n",
    "    interval=500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "N = 3\n",
    "CELL_SIZE = 100\n",
    "WINDOW_SIZE = (N * CELL_SIZE, N * CELL_SIZE)\n",
    "screen = pygame.display.set_mode(WINDOW_SIZE)\n",
    "pygame.display.set_caption(\"Maze Evolution\")\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "def draw_maze(maze, N):\n",
    "    screen.fill(WHITE)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if maze[i * N + j] == 'X':\n",
    "                pygame.draw.rect(screen, BLACK, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Example usage\n",
    "maze = [' '] * (N * N)  # Initialize an empty maze\n",
    "\n",
    "# Simulate maze evolution\n",
    "for step in range(5):\n",
    "    draw_maze(maze, N)  # Draw the current state of the maze\n",
    "    time.sleep(1)  # Simulate time passing\n",
    "    maze[step] = 'X'  # Update the maze\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "N = 3\n",
    "CELL_SIZE = 100\n",
    "MAZE_WIDTH = N * CELL_SIZE\n",
    "DESCRIPTION_WIDTH = 400\n",
    "WINDOW_SIZE = (MAZE_WIDTH + DESCRIPTION_WIDTH, N * CELL_SIZE)\n",
    "screen = pygame.display.set_mode(WINDOW_SIZE)\n",
    "pygame.display.set_caption(\"Maze Evolution with Description\")\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "GRAY = (200, 200, 200)\n",
    "\n",
    "# Fonts\n",
    "size = 20 \n",
    "font = pygame.font.Font(None, size)\n",
    "\n",
    "def draw_maze(maze, N, description):\n",
    "    # Clear the screen\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    # Draw the maze\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if maze[i * N + j] == 'X':\n",
    "                pygame.draw.rect(screen, BLACK, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "            # Draw grid lines\n",
    "            pygame.draw.rect(screen, GRAY, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE), 1)\n",
    "\n",
    "    # Draw the description\n",
    "    description_x = MAZE_WIDTH + 20  # Start description 20 pixels to the right of the maze\n",
    "    description_y = 20\n",
    "    for line in description:\n",
    "        text_surface = font.render(line, True, BLACK)\n",
    "        screen.blit(text_surface, (description_x, description_y))\n",
    "        description_y +=  size + 4 # Move down for the next line\n",
    "\n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Example usage\n",
    "maze = [' '] * (N * N)  # Initialize an empty maze\n",
    "description = []  # List to store description lines\n",
    "\n",
    "# Simulate maze evolution\n",
    "for step in range(5):\n",
    "    # Update the maze\n",
    "    maze[step] = 'X'\n",
    "\n",
    "    # Update the description\n",
    "    i = step // N  # Row index\n",
    "    j = step % N   # Column index\n",
    "    description.append(f\"From ({i}, {j}) to ({i}, {j + 1})\")\n",
    "\n",
    "    # Draw the maze and description\n",
    "    draw_maze(maze, N, description)\n",
    "    time.sleep(1)  # Simulate time passing\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
