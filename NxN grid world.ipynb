{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "## Implementing on and off-policy algorithms for learning the optimal policy for exiting a maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "player = \"P\"\n",
    "goal = \"G\"\n",
    "initial_state = 0\n",
    "final_state = N**2 - 1\n",
    "\n",
    "\n",
    "maze = [0 for i in range(0,N**2)]\n",
    "maze[initial_state] = player\n",
    "maze[final_state] = goal\n",
    "visited_states = [initial_state]\n",
    "\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "learning_history = [maze.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_maze(maze):\n",
    "    print(\"\")\n",
    "    print(maze[:N])\n",
    "    for i in range(1,N-1):\n",
    "        print(maze[i*N:(i+1)*N])\n",
    "    print(maze[N**2-N:])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_position(maze, visited_states, learning_history):\n",
    "    maze = [0 for i in range(0,N**2)]\n",
    "    maze[initial_state] = player\n",
    "    maze[final_state] = goal\n",
    "    visited_states = [initial_state]\n",
    "    learning_history = [maze.copy()]\n",
    "    return maze, visited_states, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze:\n",
      "\n",
      "['P', 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"maze:\")\n",
    "print_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 1\n",
    "wall_reward = -1\n",
    "repeated_state_reward = -1\n",
    "step_reward = -0.1\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_model(N):\n",
    "    positions = range(0,N**2)\n",
    "    transition_model = {position: {} for position in positions}\n",
    "\n",
    "    for position in positions:\n",
    "        transition_model[position][\"up\"] = position - N\n",
    "        transition_model[position][\"down\"] = position + N\n",
    "        transition_model[position][\"right\"] = position + 1\n",
    "        transition_model[position][\"left\"] = position - 1\n",
    "        if position < N:\n",
    "            transition_model[position][\"up\"] = position\n",
    "        if position in range(N**2 - N, N**2):\n",
    "            transition_model[position][\"down\"] = position\n",
    "        if position % N == 0:\n",
    "            transition_model[position][\"left\"] = position\n",
    "        if (position + 1) % N == 0:\n",
    "            transition_model[position][\"right\"] = position\n",
    "    \n",
    "    return transition_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(N):\n",
    "    positions = range(0,N**2)\n",
    "    q_table = {position: {action: 0 for action in actions} for position in positions}\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = build_transition_model(N)\n",
    "q_table = build_q_table(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(current_state, verbose=True):\n",
    "    # let's choose an action with Îµ greedy\n",
    "    if verbose:\n",
    "        print(f\"q scores: {q_table.get(current_state)}\")\n",
    "        \n",
    "    epsilon = 0.05\n",
    "    sample = random.uniform(0,1)\n",
    " \n",
    "    next_action = random.choice(actions) # setting an initial random action\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    \n",
    "    # for now my next_action is the best according to greedy selection.\n",
    "    \n",
    "    if sample > epsilon:\n",
    "        # greedy case\n",
    "        next_action = next_action\n",
    "        if verbose:\n",
    "            print(f\"selected action: {next_action} (greedy)\")\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        actions_without_the_best = [action for action in actions if action != next_action]\n",
    "        next_action = random.choice(actions_without_the_best)\n",
    "        best_q_value = q_table.get(current_state).get(next_action)\n",
    "        if verbose:\n",
    "            print(f\"selected action: {next_action} (random)\")\n",
    "    \n",
    "    return next_action, best_q_value\n",
    "    \n",
    "\n",
    "def get_reward(current_state, new_state):\n",
    "    if new_state == final_state:\n",
    "        reward = goal_reward\n",
    "        reward_message = \"I reached the goal!\"\n",
    "\n",
    "    elif current_state == new_state:\n",
    "        reward = wall_reward\n",
    "        reward_message = \"I hit the wall!\"\n",
    "\n",
    "    elif new_state in visited_states:\n",
    "        reward = repeated_state_reward\n",
    "        reward_message = \"I have already visited this state!\"\n",
    "    \n",
    "    elif current_state != new_state:\n",
    "        reward = step_reward\n",
    "        reward_message = \"I made a step!\"\n",
    "\n",
    "    else:\n",
    "        print(\"what is going on???\")\n",
    "        exit(0)\n",
    "        \n",
    "    \n",
    "    return reward, reward_message\n",
    "\n",
    "def move_player(current_state, new_state):\n",
    "    maze[current_state] = 0\n",
    "    maze[new_state] = player\n",
    "    learning_history.append(maze.copy())\n",
    "    if new_state not in visited_states:\n",
    "        visited_states.append(new_state)\n",
    "\n",
    "\n",
    "def get_best_q_value(current_state):\n",
    "    next_action = actions[0]\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions[1:]:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    return best_q_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the q-learning algorithm\n",
      "\n",
      "['P', 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 0\n",
      "I'm in position 0 and the episode just started.\n",
      "q scores: {'up': -1.27164616484375, 'down': -1.042746875, 'left': -0.996375, 'right': -0.7148121420996141}\n",
      "selected action: up (random)\n",
      "\n",
      "['P', 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 1\n",
      "I moved to the position 0\n",
      "I hit the wall! reward: -1\n",
      "q scores: {'up': -1.27164616484375, 'down': -1.042746875, 'left': -0.996375, 'right': -0.7148121420996141}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 'P', 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 2\n",
      "I moved to the position 1\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.996375, 'down': -0.6498641257812499, 'left': -0.975, 'right': -0.6498641257812499}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 'P', 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 3\n",
      "I moved to the position 2\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -1.42625, 'down': -0.9785656249999999, 'left': -0.99875, 'right': -1.04161875}\n",
      "selected action: down (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 'P', 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 4\n",
      "I moved to the position 7\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.75, 'down': -0.99875, 'left': -0.975, 'right': -0.6507268593750001}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 'P', 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 5\n",
      "I moved to the position 8\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -1.04161875, 'down': -0.99875, 'left': -0.975, 'right': -0.6420995234375}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'P']\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 6\n",
      "I moved to the position 9\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.9931875000000001, 'down': -0.9892828125000002, 'left': -1.42625, 'right': -1.4375}\n",
      "selected action: down (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'P']\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 7\n",
      "I moved to the position 14\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.975, 'down': -0.5487500000000001, 'left': -0.5487500000000001, 'right': -0.975}\n",
      "selected action: right (random)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'P']\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 8\n",
      "I moved to the position 14\n",
      "I hit the wall! reward: -1\n",
      "q scores: {'up': -0.975, 'down': -0.5487500000000001, 'left': -0.5487500000000001, 'right': -0.975}\n",
      "selected action: left (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 'P', 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 9\n",
      "I moved to the position 13\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.975, 'down': -0.5487500000000001, 'left': -0.525, 'right': -0.5475}\n",
      "selected action: left (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 'P', 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 10\n",
      "I moved to the position 12\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.5475, 'down': -0.142625, 'left': -0.5, 'right': -0.5475}\n",
      "selected action: down (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 'P', 0, 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 11\n",
      "I moved to the position 17\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.5, 'down': -0.142625, 'left': -0.5, 'right': -0.142625}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 'P', 0]\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 12\n",
      "I moved to the position 18\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.5, 'down': -0.5475, 'left': -0.5, 'right': -0.18549375}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'P']\n",
      "[0, 0, 0, 0, 'G']\n",
      "\n",
      "timestep: 13\n",
      "I moved to the position 19\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': 0, 'down': 2.6490810937500004, 'left': -0.5, 'right': 0}\n",
      "selected action: down (greedy)\n",
      "\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 'P']\n",
      "\n",
      "timestep: 14\n",
      "I moved to the position 24\n",
      "I reached the goal! reward: 1\n",
      "\n",
      "Episode time (iterations): 14\n"
     ]
    }
   ],
   "source": [
    "# algorithm = \"sarsa\"\n",
    "algorithm = \"q-learning\"\n",
    "\n",
    "print(f\"Running the {algorithm} algorithm\")\n",
    "\n",
    "maze, visited_states, learning_history = reset_position(maze, visited_states, learning_history)\n",
    "\n",
    "print_maze(maze)\n",
    "print(\"timestep: 0\")\n",
    "\n",
    "training_time = range(0,1000)\n",
    "current_state = initial_state\n",
    "print(f\"I'm in position {current_state} and the episode just started.\")\n",
    "\n",
    "current_action, old_q_estimate = choose_action(current_state)\n",
    "# print(\"\")\n",
    "\n",
    "for t in training_time: # for each episode \n",
    "    \n",
    "    new_state = transition_model.get(current_state).get(current_action)  \n",
    "    reward, reward_message = get_reward(current_state, new_state)\n",
    "    move_player(current_state, new_state)\n",
    "    print_maze(maze)\n",
    "    \n",
    "    print(f\"timestep: {t+1}\")\n",
    "    print(f\"I moved to the position {new_state}\")\n",
    "    print(f\"{reward_message} reward: {reward}\")\n",
    "\n",
    "    new_action, next_q_estimate = choose_action(new_state, verbose=(not new_state==final_state))\n",
    "\n",
    "    if algorithm == \"sarsa\":\n",
    "        updated_q_estimate = old_q_estimate + alpha*(reward + gamma*next_q_estimate - old_q_estimate)\n",
    "    if algorithm == \"q-learning\":\n",
    "        updated_q_estimate = old_q_estimate + alpha*(reward + gamma*get_best_q_value(current_state) - old_q_estimate)\n",
    "\n",
    "    q_table[current_state][current_action] = updated_q_estimate\n",
    "\n",
    "    current_state = new_state\n",
    "    current_action, old_q_estimate = new_action, next_q_estimate\n",
    "\n",
    "    if current_state == N**2-1:\n",
    "        break\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Episode time (iterations): {t+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
