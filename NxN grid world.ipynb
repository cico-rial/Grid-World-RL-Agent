{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "## Implementing on and off-policy algorithms for learning the optimal policy for exiting a maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.4)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "player = \"P\"\n",
    "goal = \"G\"\n",
    "initial_state = 0\n",
    "final_state = N**2 - 1\n",
    "\n",
    "\n",
    "maze = [0 for i in range(0,N**2)]\n",
    "maze[initial_state] = player\n",
    "maze[final_state] = goal\n",
    "visited_states = [initial_state]\n",
    "\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "learning_history = [maze.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_maze(maze):\n",
    "    print(\"\")\n",
    "    print(maze[:N])\n",
    "    for i in range(1,N-1):\n",
    "        print(maze[i*N:(i+1)*N])\n",
    "    print(maze[N**2-N:])\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def draw_maze(maze, description):\n",
    "    # Clear the screen\n",
    "    screen.fill(WHITE)\n",
    "\n",
    "    # Draw the maze\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i * N + j in visited_states:\n",
    "                pygame.draw.rect(screen, GRAY, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "            if maze[i * N + j] == 'P':\n",
    "                pygame.draw.rect(screen, BLACK, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "            if maze[i * N + j] == 'G':\n",
    "                pygame.draw.rect(screen, RED, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE))\n",
    "            # Draw grid lines\n",
    "            pygame.draw.rect(screen, GRAY, (j * CELL_SIZE, i * CELL_SIZE, CELL_SIZE, CELL_SIZE), 1)\n",
    "\n",
    "    # Draw the description\n",
    "    description_x = MAZE_WIDTH + 20  # Start description 20 pixels to the right of the maze\n",
    "    description_y = 10\n",
    "    for i, line in enumerate(description):\n",
    "        text_surface = font.render(line, True, BLACK)\n",
    "        screen.blit(text_surface, (description_x, description_y))\n",
    "        description_y +=  size + 4 # Move down for the next line\n",
    "        pygame.display.flip() # Update the display\n",
    "        if (len(description) < 7) or (i>6 and len(description) > 6): \n",
    "            time.sleep(0.2)\n",
    "\n",
    "    # Update the display\n",
    "    # pygame.display.flip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_position(maze, visited_states, learning_history):\n",
    "    maze = [0 for i in range(0,N**2)]\n",
    "    maze[initial_state] = player\n",
    "    maze[final_state] = goal\n",
    "    visited_states = [initial_state]\n",
    "    learning_history = [maze.copy()]\n",
    "    return maze, visited_states, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze:\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"maze:\")\n",
    "print_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 1\n",
    "wall_reward = -1\n",
    "repeated_state_reward = -1\n",
    "step_reward = -0.1\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_model(N):\n",
    "    positions = range(0,N**2)\n",
    "    transition_model = {position: {} for position in positions}\n",
    "\n",
    "    for position in positions:\n",
    "        transition_model[position][\"up\"] = position - N\n",
    "        transition_model[position][\"down\"] = position + N\n",
    "        transition_model[position][\"right\"] = position + 1\n",
    "        transition_model[position][\"left\"] = position - 1\n",
    "        if position < N:\n",
    "            transition_model[position][\"up\"] = position\n",
    "        if position in range(N**2 - N, N**2):\n",
    "            transition_model[position][\"down\"] = position\n",
    "        if position % N == 0:\n",
    "            transition_model[position][\"left\"] = position\n",
    "        if (position + 1) % N == 0:\n",
    "            transition_model[position][\"right\"] = position\n",
    "\n",
    "    return transition_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(N):\n",
    "    positions = range(0,N**2)\n",
    "    q_table = {position: {action: 0 for action in actions} for position in positions}\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = build_transition_model(N)\n",
    "q_table = build_q_table(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(current_state, verbose=True):\n",
    "    log_print = []\n",
    "\n",
    "    # let's choose an action with Îµ greedy\n",
    "    if verbose:\n",
    "        # print(f\"q scores: {q_table.get(current_state)}\")\n",
    "        log_print.append(f\"q scores: {q_table.get(current_state)}\")\n",
    "    \n",
    "    epsilon = 0.05\n",
    "    sample = random.uniform(0,1)\n",
    " \n",
    "    next_action = random.choice(actions) # setting an initial random action\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    \n",
    "    # for now my next_action is the best according to greedy selection.\n",
    "    \n",
    "    if sample > epsilon:\n",
    "        # greedy case\n",
    "        next_action = next_action\n",
    "        if verbose:\n",
    "            # print(f\"greedy action: {next_action}\")\n",
    "            log_print.append(f\"greedy action: {next_action}\")\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        actions_without_the_best = [action for action in actions if action != next_action]\n",
    "        next_action = random.choice(actions_without_the_best)\n",
    "        best_q_value = q_table.get(current_state).get(next_action)\n",
    "        if verbose:\n",
    "            # print(f\"random action: {next_action}\")\n",
    "            log_print.append(f\"random action: {next_action}\")\n",
    "    \n",
    "    # print(\"\")\n",
    "    return next_action, best_q_value, log_print\n",
    "    \n",
    "\n",
    "def get_reward(current_state, new_state):\n",
    "    log_print = \"\"\n",
    "    if new_state == final_state:\n",
    "        reward = goal_reward\n",
    "        # print(\"I reached the goal!\")\n",
    "        log_print += \"I reached the goal!\"\n",
    "\n",
    "    elif current_state == new_state:\n",
    "        reward = wall_reward\n",
    "        # print(\"I hit the wall!\")\n",
    "        log_print += \"I hit the wall!\"\n",
    "\n",
    "    elif new_state in visited_states:\n",
    "        # print(\"I have already visited this state!\")\n",
    "        reward = repeated_state_reward\n",
    "        log_print += \"I have already visited this state!\"\n",
    "    \n",
    "    elif current_state != new_state:\n",
    "        # print(\"I made a step!\")\n",
    "        reward = step_reward\n",
    "        log_print += \"I made a step!\"\n",
    "\n",
    "    else:\n",
    "        print(\"what is going on???\")\n",
    "        exit(0)\n",
    "        \n",
    "    \n",
    "    return reward, [log_print]\n",
    "\n",
    "def move_player(current_state, new_state):\n",
    "    maze[current_state] = 0\n",
    "    maze[new_state] = player\n",
    "    learning_history.append(maze.copy())\n",
    "    if new_state not in visited_states:\n",
    "        visited_states.append(new_state)\n",
    "\n",
    "\n",
    "def get_best_q_value(current_state):\n",
    "    next_action = actions[0]\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions[1:]:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    return best_q_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 0\n",
      "I'm in position 0 and the episode just started.\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': -0.05}\n",
      "greedy action: up\n",
      "\n",
      "timestep: 1\n",
      "I moved to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': -0.05}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 2\n",
      "I moved to the position 3\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': -0.5}\n",
      "greedy action: up\n",
      "\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 2\n",
      "I moved to the position 3\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': -0.5}\n",
      "greedy action: up\n",
      "\n",
      "timestep: 3\n",
      "I moved to the position 0\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 4\n",
      "I moved to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 4\n",
      "I moved to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "timestep: 5\n",
      "I moved to the position 0\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': -0.5, 'right': -0.05}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 6\n",
      "I moved to the position 3\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': -0.5, 'right': -0.5}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "timestep: 6\n",
      "I moved to the position 3\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': -0.5, 'right': -0.5}\n",
      "greedy action: down\n",
      "\n",
      "timestep: 7\n",
      "I moved to the position 6\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: up\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "['P', 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 8\n",
      "I moved to the position 3\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': -0.5, 'right': -0.5}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "timestep: 8\n",
      "I moved to the position 3\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.05, 'left': -0.5, 'right': -0.5}\n",
      "greedy action: down\n",
      "\n",
      "timestep: 9\n",
      "I moved to the position 6\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "timestep: 10\n",
      "I moved to the position 6\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 'P', 'G']\n",
      "\n",
      "timestep: 10\n",
      "I moved to the position 6\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "timestep: 11\n",
      "I moved to the position 7\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 'P', 'G']\n",
      "\n",
      "timestep: 12\n",
      "I moved to the position 7\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "timestep: 12\n",
      "I moved to the position 7\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
      "greedy action: left\n",
      "\n",
      "timestep: 13\n",
      "I moved to the position 6\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.5, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "timestep: 14\n",
      "I moved to the position 6\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.5, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "['P', 0, 'G']\n",
      "\n",
      "timestep: 14\n",
      "I moved to the position 6\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.5, 'left': 0, 'right': -0.05}\n",
      "greedy action: left\n",
      "\n",
      "timestep: 15\n",
      "I moved to the position 6\n",
      "I hit the wall!\n",
      "reward: -1\n",
      "q scores: {'up': -0.5, 'down': -0.5, 'left': -0.5, 'right': -0.05}\n",
      "greedy action: right\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 'P', 'G']\n",
      "\n",
      "timestep: 16\n",
      "I moved to the position 7\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': 0, 'down': -0.5, 'left': -0.5, 'right': 0}\n",
      "greedy action: up\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 'P', 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 16\n",
      "I moved to the position 7\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': 0, 'down': -0.5, 'left': -0.5, 'right': 0}\n",
      "greedy action: up\n",
      "\n",
      "timestep: 17\n",
      "I moved to the position 4\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "q scores: {'up': -0.5, 'down': 0, 'left': -0.05, 'right': -0.05}\n",
      "greedy action: down\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 'P', 'G']\n",
      "\n",
      "timestep: 18\n",
      "I moved to the position 7\n",
      "I have already visited this state!\n",
      "reward: -1\n",
      "q scores: {'up': -0.05, 'down': -0.5, 'left': -0.5, 'right': 0}\n",
      "greedy action: right\n",
      "\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'P']\n",
      "\n",
      "Episode time (iterations): 18\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "# N = 3\n",
    "CELL_SIZE = 100\n",
    "MAZE_WIDTH = N * CELL_SIZE\n",
    "DESCRIPTION_WIDTH = 400\n",
    "WINDOW_SIZE = (MAZE_WIDTH + DESCRIPTION_WIDTH, max(N * CELL_SIZE, 320))\n",
    "screen = pygame.display.set_mode(WINDOW_SIZE)\n",
    "pygame.display.set_caption(\"Maze Evolution with Description\")\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "GRAY = (200, 200, 200)\n",
    "RED = (255, 0, 0)\n",
    "\n",
    "# Fonts\n",
    "size = 20 \n",
    "font = pygame.font.Font(None, size)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "maze, visited_states, learning_history = reset_position(maze, visited_states, learning_history)\n",
    "print_maze(maze)\n",
    "\n",
    "# algorithm = \"sarsa\"\n",
    "algorithm = \"q-learning\"\n",
    "\n",
    "training_time = range(0,1000)\n",
    "current_state = initial_state\n",
    "current_action, old_q_estimate, log_action = choose_action(current_state)\n",
    "\n",
    "log_print = [\"timestep: 0\"]\n",
    "log_print.append(f\"I'm in position {current_state} and the episode just started.\")\n",
    "log_print.extend(log_action)\n",
    "log_print.append(\"\")\n",
    "\n",
    "draw_maze(maze, log_print)\n",
    "time.sleep(2)\n",
    "# log_print=[]\n",
    "\n",
    "for t in training_time: # for each episode\n",
    "    # choose the action according to the q_table\n",
    "    log_print.append(f\"timestep: {t+1}\")\n",
    "    new_state = transition_model.get(current_state).get(current_action)\n",
    "    log_print.append(f\"I moved to the position {new_state}\")\n",
    "    reward, log_reward = get_reward(current_state, new_state)\n",
    "    log_print.extend(log_reward)\n",
    "    log_print.append(f\"reward: {reward}\")\n",
    "    \n",
    "    move_player(current_state, new_state)\n",
    "    print_maze(maze)\n",
    "\n",
    "    new_action, next_q_estimate, log_action = choose_action(new_state, verbose=(not new_state==final_state))\n",
    "    log_print.extend(log_action)\n",
    "\n",
    "    if algorithm == \"sarsa\":\n",
    "        updated_q_estimate = round(old_q_estimate + alpha*(reward + gamma*next_q_estimate - old_q_estimate), 3)\n",
    "    if algorithm == \"q-learning\":\n",
    "        updated_q_estimate = round(old_q_estimate + alpha*(reward + gamma*get_best_q_value(current_state) - old_q_estimate), 3)\n",
    "\n",
    "    q_table[current_state][current_action] = updated_q_estimate\n",
    "\n",
    "    current_state = new_state\n",
    "    current_action, old_q_estimate = new_action, next_q_estimate\n",
    "\n",
    "    draw_maze(maze, log_print)\n",
    "    time.sleep(2)  # Simulate time passing\n",
    "\n",
    "    if current_state == N**2-1:\n",
    "        break\n",
    "    \n",
    "    log_print.append(f\"\")\n",
    "\n",
    "    for log in log_print:\n",
    "        print(log)\n",
    "    \n",
    "    if t % 2 == 0:\n",
    "        log_print = []\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()\n",
    "\n",
    "print(f\"Episode time (iterations): {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
