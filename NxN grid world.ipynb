{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning \n",
    "## Implementing on and off-policy algorithms for learning the optimal policy for exiting a maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "\n",
    "player = \"P\"\n",
    "goal = \"G\"\n",
    "initial_state = 0\n",
    "final_state = N**2 - 1\n",
    "\n",
    "\n",
    "maze = [0 for i in range(0,N**2)]\n",
    "maze[initial_state] = player\n",
    "maze[final_state] = goal\n",
    "visited_states = [initial_state]\n",
    "\n",
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "learning_history = [maze.copy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_maze(maze):\n",
    "    print(\"\")\n",
    "    print(maze[:N])\n",
    "    for i in range(1,N-1):\n",
    "        print(maze[i*N:(i+1)*N])\n",
    "    print(maze[N**2-N:])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_position(maze, visited_states, learning_history):\n",
    "    maze = [0 for i in range(0,N**2)]\n",
    "    maze[initial_state] = player\n",
    "    maze[final_state] = goal\n",
    "    visited_states = [initial_state]\n",
    "    learning_history = [maze.copy()]\n",
    "    return maze, visited_states, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze:\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"maze:\")\n",
    "print_maze(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 1\n",
    "wall_reward = -1\n",
    "repeated_state_reward = -1\n",
    "step_reward = -0.1\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_model(N):\n",
    "    positions = range(0,N**2)\n",
    "    transition_model = {position: {} for position in positions}\n",
    "\n",
    "    for position in positions:\n",
    "        transition_model[position][\"up\"] = position - N\n",
    "        transition_model[position][\"down\"] = position + N\n",
    "        transition_model[position][\"right\"] = position + 1\n",
    "        transition_model[position][\"left\"] = position - 1\n",
    "        if position < N:\n",
    "            transition_model[position][\"up\"] = position\n",
    "        if position in range(N**2 - N, N**2):\n",
    "            transition_model[position][\"down\"] = position\n",
    "        if position % N == 0:\n",
    "            transition_model[position][\"left\"] = position\n",
    "        if (position + 1) % N == 0:\n",
    "            transition_model[position][\"right\"] = position\n",
    "    \n",
    "    return transition_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(N):\n",
    "    positions = range(0,N**2)\n",
    "    q_table = {position: {action: 0 for action in actions} for position in positions}\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = build_transition_model(N)\n",
    "q_table = build_q_table(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(current_state, verbose=True):\n",
    "    # let's choose an action with Îµ greedy\n",
    "    if verbose:\n",
    "        print(f\"q scores: {q_table.get(current_state)}\")\n",
    "        \n",
    "    epsilon = 0.05\n",
    "    sample = random.uniform(0,1)\n",
    " \n",
    "    next_action = random.choice(actions) # setting an initial random action\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    \n",
    "    # for now my next_action is the best according to greedy selection.\n",
    "    \n",
    "    if sample > epsilon:\n",
    "        # greedy case\n",
    "        next_action = next_action\n",
    "        if verbose:\n",
    "            print(f\"selected action: {next_action} (greedy)\")\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        actions_without_the_best = [action for action in actions if action != next_action]\n",
    "        next_action = random.choice(actions_without_the_best)\n",
    "        best_q_value = q_table.get(current_state).get(next_action)\n",
    "        if verbose:\n",
    "            print(f\"selected action: {next_action} (random)\")\n",
    "    \n",
    "    return next_action, best_q_value\n",
    "    \n",
    "\n",
    "def get_reward(current_state, new_state):\n",
    "    if new_state == final_state:\n",
    "        reward = goal_reward\n",
    "        reward_message = \"I reached the goal!\"\n",
    "\n",
    "    elif current_state == new_state:\n",
    "        reward = wall_reward\n",
    "        reward_message = \"I hit the wall!\"\n",
    "\n",
    "    elif new_state in visited_states:\n",
    "        reward = repeated_state_reward\n",
    "        reward_message = \"I have already visited this state!\"\n",
    "    \n",
    "    elif current_state != new_state:\n",
    "        reward = step_reward\n",
    "        reward_message = \"I made a step!\"\n",
    "\n",
    "    else:\n",
    "        print(\"what is going on???\")\n",
    "        exit(0)\n",
    "        \n",
    "    \n",
    "    return reward, reward_message\n",
    "\n",
    "def move_player(current_state, new_state):\n",
    "    maze[current_state] = 0\n",
    "    maze[new_state] = player\n",
    "    learning_history.append(maze.copy())\n",
    "    if new_state not in visited_states:\n",
    "        visited_states.append(new_state)\n",
    "\n",
    "\n",
    "def get_best_q_value(current_state):\n",
    "    next_action = actions[0]\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # picking the q value associated to the action \n",
    "\n",
    "    for action in actions[1:]:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    return best_q_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the q-learning algorithm\n",
      "\n",
      "['P', 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 0\n",
      "I'm in position 0 and the episode just started.\n",
      "q scores: {'up': -0.5, 'down': -0.592625, 'left': -0.5, 'right': -0.0975}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 'P', 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 1\n",
      "I moved to the position 1 (0-->1)\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': -0.5, 'down': -0.5475, 'left': -0.5, 'right': -0.05}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 'P']\n",
      "[0, 0, 0]\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 2\n",
      "I moved to the position 2 (1-->2)\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': 0}\n",
      "selected action: down (greedy)\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 'P']\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 3\n",
      "I moved to the position 5 (2-->5)\n",
      "I made a step! reward: -0.1\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': 0}\n",
      "selected action: right (greedy)\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 'P']\n",
      "[0, 0, 'G']\n",
      "\n",
      "timestep: 4\n",
      "I moved to the position 5 (5-->5)\n",
      "I hit the wall! reward: -1\n",
      "q scores: {'up': 0, 'down': 0, 'left': -0.5, 'right': 0}\n",
      "selected action: down (greedy)\n",
      "\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 'P']\n",
      "\n",
      "timestep: 5\n",
      "I moved to the position 8 (5-->8)\n",
      "I reached the goal! reward: 1\n",
      "\n",
      "Episode time (iterations): 5\n"
     ]
    }
   ],
   "source": [
    "# algorithm = \"sarsa\"\n",
    "algorithm = \"q-learning\"\n",
    "\n",
    "print(f\"Running the {algorithm} algorithm\")\n",
    "\n",
    "maze, visited_states, learning_history = reset_position(maze, visited_states, learning_history)\n",
    "\n",
    "print_maze(maze)\n",
    "print(\"timestep: 0\")\n",
    "\n",
    "training_time = range(0,1000)\n",
    "current_state = initial_state\n",
    "print(f\"I'm in position {current_state} and the episode just started.\")\n",
    "\n",
    "current_action, old_q_estimate = choose_action(current_state)\n",
    "# print(\"\")\n",
    "\n",
    "for t in training_time: # for each episode \n",
    "    \n",
    "    new_state = transition_model.get(current_state).get(current_action)  \n",
    "    reward, reward_message = get_reward(current_state, new_state)\n",
    "    move_player(current_state, new_state)\n",
    "    print_maze(maze)\n",
    "    \n",
    "    print(f\"timestep: {t+1}\")\n",
    "    print(f\"I moved to the position {new_state} ({current_state}-->{new_state})\")\n",
    "    print(f\"{reward_message} reward: {reward}\")\n",
    "\n",
    "    new_action, next_q_estimate = choose_action(new_state, verbose=(not new_state==final_state))\n",
    "\n",
    "    if algorithm == \"sarsa\":\n",
    "        updated_q_estimate = old_q_estimate + alpha*(reward + gamma*next_q_estimate - old_q_estimate)\n",
    "    if algorithm == \"q-learning\":\n",
    "        updated_q_estimate = old_q_estimate + alpha*(reward + gamma*get_best_q_value(current_state) - old_q_estimate)\n",
    "\n",
    "    q_table[current_state][current_action] = updated_q_estimate\n",
    "\n",
    "    current_state = new_state\n",
    "    current_action, old_q_estimate = new_action, next_q_estimate\n",
    "\n",
    "    if current_state == N**2-1:\n",
    "        break\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Episode time (iterations): {t+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
