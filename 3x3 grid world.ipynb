{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning practice\n",
    "## Implementing the SARSA on-policy algorithm for learning the optimal policy\n",
    "Let's consider a simple 3x3 grid world scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: [-1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "learning history: [[-1, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "player = -1\n",
    "goal = 1\n",
    "initial_state = 0\n",
    "final_state = 8\n",
    "\n",
    "states = [0 for i in range(0,9)]\n",
    "states[initial_state] = player\n",
    "states[final_state] = goal\n",
    "visited_states = [initial_state]\n",
    "learning_history = [states.copy()]\n",
    "print(f\"states: {states}\")\n",
    "print(f\"learning history: {learning_history}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "goal_reward = 1\n",
    "wall_reward = -1\n",
    "repeated_state_reward = -1\n",
    "step_reward = -0.1\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_position(states, visited_states, learning_history):\n",
    "    states = [0 for i in range(0,9)]\n",
    "    states[initial_state] = player\n",
    "    states[final_state] = goal\n",
    "    visited_states = [initial_state]\n",
    "    learning_history = [states.copy()]\n",
    "    return states, visited_states, learning_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_model = {\n",
    "    0: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 3,\n",
    "        \"right\": 1,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    1: {\n",
    "        \"up\": 1,\n",
    "        \"down\": 4,\n",
    "        \"right\": 2,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    2: {\n",
    "        \"up\": 2,\n",
    "        \"down\": 5,\n",
    "        \"right\": 2,\n",
    "        \"left\": 1\n",
    "    },\n",
    "    3: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 6,\n",
    "        \"right\": 4,\n",
    "        \"left\": 3\n",
    "    },\n",
    "    4: {\n",
    "        \"up\": 1,\n",
    "        \"down\": 7,\n",
    "        \"right\": 5,\n",
    "        \"left\": 3\n",
    "    },\n",
    "    5: {\n",
    "        \"up\": 2,\n",
    "        \"down\": 8,\n",
    "        \"right\": 5,\n",
    "        \"left\": 4\n",
    "    },\n",
    "    6: {\n",
    "        \"up\": 3,\n",
    "        \"down\": 6,\n",
    "        \"right\": 7,\n",
    "        \"left\": 6\n",
    "    },\n",
    "    7: {\n",
    "        \"up\": 4,\n",
    "        \"down\": 7,\n",
    "        \"right\": 8,\n",
    "        \"left\": 6\n",
    "    },\n",
    "    8: {\n",
    "        \"up\": 5,\n",
    "        \"down\": 8,\n",
    "        \"right\": 8,\n",
    "        \"left\": 7\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = {\n",
    "    0: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    1: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    2: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    3: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    4: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    5: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    6: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    7: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    },\n",
    "    8: {\n",
    "        \"up\": 0,\n",
    "        \"down\": 0,\n",
    "        \"right\": 0,\n",
    "        \"left\": 0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(current_state):\n",
    "    # let's choose an action with Îµ greedy\n",
    "    print(f\"q scores: {q_table.get(current_state)}\")\n",
    "    \n",
    "    epsilon = 0.05\n",
    "    sample = random.uniform(0,1)\n",
    " \n",
    "    next_action = \"up\" # setting an initial arbitrary value\n",
    "    best_q_value = q_table.get(current_state).get(next_action) # setting an initial value \n",
    "\n",
    "    for action in actions:\n",
    "        if q_table.get(current_state).get(action) > best_q_value:\n",
    "            next_action = action\n",
    "            best_q_value = q_table.get(current_state).get(next_action)\n",
    "    \n",
    "    # for now my next_action is the best according to greedy selection.\n",
    "    \n",
    "    if sample > epsilon:\n",
    "        # greedy case\n",
    "        next_action = next_action\n",
    "        print(f\"greedy action: {next_action}\")\n",
    "        \n",
    "    else:\n",
    "        actions_without_the_best = [action for action in actions if action != next_action]\n",
    "        next_action = random.choice(actions_without_the_best)\n",
    "        best_q_value = q_table.get(current_state).get(next_action)\n",
    "        print(f\"random action: {next_action}\")\n",
    "    \n",
    "\n",
    "    return next_action, best_q_value\n",
    "    \n",
    "\n",
    "def get_reward(current_state, new_state):\n",
    "    if current_state == final_state:\n",
    "        reward = goal_reward\n",
    "        print(\"I reached the goal!\")\n",
    "\n",
    "    elif current_state == new_state:\n",
    "        reward = wall_reward\n",
    "        print(\"I hit the wall!\")\n",
    "\n",
    "    elif new_state in visited_states:\n",
    "        print(\"I have already visited this state!\")\n",
    "        reward = repeated_state_reward\n",
    "    \n",
    "    elif current_state != new_state:\n",
    "        print(\"I made a step!\")\n",
    "        reward = step_reward\n",
    "\n",
    "    else:\n",
    "        print(\"what is going on???\")\n",
    "        exit(0)\n",
    "    \n",
    "    return reward\n",
    "    \n",
    "\n",
    "def move_player(current_state, new_state):\n",
    "    # print(f\"current state: {current_state}\")\n",
    "    # print(f\"new state: {new_state}\")\n",
    "\n",
    "    states[current_state] = 0\n",
    "    states[new_state] = -1\n",
    "    learning_history.append(states.copy())\n",
    "    if new_state not in visited_states:\n",
    "        visited_states.append(new_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_maze(states):\n",
    "    print(states[:3])\n",
    "    print(states[3:6])\n",
    "    print(states[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, 1]\n",
      "q scores: {'up': -0.975, 'down': -0.343899998589838, 'right': -0.7047098046875, 'left': -1.2290905687093736}\n",
      "greedy action: down\n",
      "I'm in position 0. I will select down\n",
      "I'm moving to the position 3\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "[0, 0, 0]\n",
      "[-1, 0, 0]\n",
      "[0, 0, 1]\n",
      "q scores: {'up': -0.7725, 'down': -0.525, 'right': -0.27099999821966053, 'left': -0.975}\n",
      "greedy action: right\n",
      "From 3, i select right as the next action\n",
      "I'm moving to the position 4\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "[0, 0, 0]\n",
      "[0, -1, 0]\n",
      "[0, 0, 1]\n",
      "q scores: {'up': -0.5, 'down': -0.1899999998137355, 'right': -0.344912109375, 'left': -0.5225}\n",
      "greedy action: down\n",
      "From 4, i select down as the next action\n",
      "I'm moving to the position 7\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, -1, 1]\n",
      "q scores: {'up': -0.5, 'down': -0.5, 'right': -0.09999999999417925, 'left': -0.725}\n",
      "greedy action: right\n",
      "From 7, i select right as the next action\n",
      "I'm moving to the position 8\n",
      "I made a step!\n",
      "reward: -0.1\n",
      "[0, 0, 0]\n",
      "[0, 0, 0]\n",
      "[0, 0, -1]\n",
      "q scores: {'up': 0, 'down': 0, 'right': 0, 'left': 0}\n",
      "greedy action: up\n",
      "From 8, i select up as the next action\n",
      "Training time (iterations): 3\n"
     ]
    }
   ],
   "source": [
    "states, visited_states, learning_history = reset_position(states, visited_states, learning_history)\n",
    "print_maze(states)\n",
    "\n",
    "training_time = range(0,1000)\n",
    "current_state = initial_state\n",
    "current_action, old_q_estimate = choose_action(current_state) # return a string containing the action and the current q_value?\n",
    "\n",
    "print(f\"I'm in position {current_state}. I will select {current_action}\")\n",
    "\n",
    "for t in training_time: # for each episode\n",
    "    # choose the action according to the q_table\n",
    "    \n",
    "    new_state = transition_model.get(current_state).get(current_action)\n",
    "    print(f\"I'm moving to the position {new_state}\")\n",
    "    reward = get_reward(current_state, new_state)\n",
    "    print(f\"reward: {reward}\")\n",
    "    \n",
    "    move_player(current_state, new_state)\n",
    "    print_maze(states)\n",
    "\n",
    "    new_action, next_q_estimate = choose_action(new_state)\n",
    "    print(f\"From {new_state}, i select {new_action} as the next action\")\n",
    "\n",
    "    updated_q_estimate = old_q_estimate + alpha*(reward + gamma*next_q_estimate - old_q_estimate)\n",
    "    q_table[current_state][current_action] = updated_q_estimate\n",
    "\n",
    "    current_state = new_state\n",
    "    current_action, old_q_estimate = new_action, next_q_estimate\n",
    "\n",
    "    if current_state == 8:\n",
    "        break\n",
    "\n",
    "print(f\"Training time (iterations): {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
